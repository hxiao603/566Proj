{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw-1HdCYM4hP",
        "outputId": "12c15119-f035-4a97-c7ce-e1ccf0142445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.14.2-py3-none-any.whl (262 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/262.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m204.8/262.4 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.3)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.14.2\n"
          ]
        }
      ],
      "source": [
        "!pip install openai\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "openai.api_key = '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'"
      ],
      "metadata": {
        "id": "v8Kf8nYAXUFD"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It is the list of question, each question has different environment, and student GPT have to explain well and the role which\n",
        "# mentioned in question can understand at real world.\n",
        "\n",
        "question_lst = [\"Explain to a 5-year-old child, Why do we need to wear seatbelts in the car?\",\"Describe to a senior citizen, How does online banking work, and why is it safe?\",\n",
        "\"Inform a high school student, What is climate change, and how does it affect our planet?\", \"Discuss with a college professor specializing in literature, How has digital media transformed the way we read and analyze texts?\",\n",
        "\"Clarify to a software engineer What is the significance of quantum computing in the future of technology?\", \"Advise a local farmer How can sustainable farming practices benefit our environment and food supply?\",\n",
        "\"Outline for an entrepreneur What are the key factors in creating a successful startup in today's digital age?\", \"Describe to an environmental scientist, What are the most effective strategies for combating plastic pollution in the oceans?\",\n",
        "\"Explain to a math student What is an embedding layer in machine learning, and how does it relate to organizing ingredients in a recipe?\"]\n",
        "\n",
        "# max socre is 123 for final score, but you can modify the weight of each features.\n",
        "# It is setting for grader,the grader GPT will know what kind of output it need to prompt, the output will be\n",
        "# 10 features score (final socre always wrong When GPT calculate it, ignore it.), and it is a string in dictionary\n",
        "# format\n",
        "text_rule_grader = \"\"\"I need you grade the answer base on the questions.\\n\n",
        "      We have 10 features: Language Clarity, Content Accuracy, Relevance to Role, Emotional Intelligence, Engagement Level, Personalization, Complexity Appropriateness, Ethical Sensitivity, Interactive Potential, Adaptability.\\n\n",
        "      Each features you will grade them in range 0 to 10, 0 means totally worst in fiels, 10 means best in fields.\\n\n",
        "      Now I also have equation: grade = 1.5*Language Clarity + 2.0*Content Accuracy + 1*Relevance to Role + 1.2*Emotional Intelligence + 1.0*Engagement Level + 1.0*Personalization + 1.5*Complexity Appropriateness + 1.3*Ethical Sensitivity + 0.8*Interactive Potential + 1.0*Adaptability\\n\n",
        "      You will only give us the final grade(just grade, you don't need to show how to calculate it.) and each scores in features in format as follow:\\n\n",
        "      {\"Language Clarity\": Language Clarity score, \"Content Accuracy\": Content Accuracy score, \"Relevance to Role\": Relevance to Role score, \"Emotional Intelligence\": Emotional Intelligence score, \"Engagement Level\": Engagement Level score, \"Personalization\": Personalization score, \"Complexity Appropriateness\": Complexity Appropriateness score,\"Ethical Sensitivity\": Ethical Sensitivity score, \"Adaptability\": Adaptability score, \"final\": final grade score}\\n\"\"\"\n"
      ],
      "metadata": {
        "id": "ZNyTh_GuUMFq"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If answer is unsatisfied, the input has each score, previous answer, and question. (we will not use loss for now).\n",
        "# And student GPT will know which question it answer, and why it is unsatisfied, and answer again.\n",
        "def generate_student_answer(question,answer,score,loss):\n",
        "  s = \"\"\"Language Clarity: {LanguageClarity},\n",
        "Content Accuracy: {ContentAccuracy},\n",
        "Relevance to Role: {RelevancetoRole},\n",
        "Emotional Intelligence: {EmotionalIntelligence},\n",
        "Engagement Level: {EngagementLevel},\n",
        "Personalization: {Personalization},\n",
        "Complexity Appropriateness: {ComplexityAppropriateness},\n",
        "Ethical Sensitivity: {EthicalSensitivity},\n",
        "Interactive Potential: {InteractivePotential},\n",
        "Adaptability: {Adaptability},\n",
        "final: {final}\"\"\".format(\n",
        "    LanguageClarity=score[\"Language Clarity\"],\n",
        "    ContentAccuracy=score[\"Content Accuracy\"],\n",
        "    RelevancetoRole=score[\"Relevance to Role\"],\n",
        "    EmotionalIntelligence=score[\"Emotional Intelligence\"],\n",
        "    EngagementLevel=score[\"Engagement Level\"],\n",
        "    Personalization=score[\"Personalization\"],\n",
        "    ComplexityAppropriateness=score[\"Complexity Appropriateness\"],\n",
        "    EthicalSensitivity=score[\"Ethical Sensitivity\"],\n",
        "    InteractivePotential=score[\"Interactive Potential\"],\n",
        "    Adaptability=score[\"Adaptability\"],\n",
        "    final=score[\"final\"]\n",
        ")\n",
        "\n",
        "  text = \"The answer is unsatified.\\n Question{question}\\n Answer:{answer}\\n and the score as follow:\\n\".format(question=question,answer=answer)+s+\"\\n , you need to rewrite the answer.\"\n",
        "  return text"
      ],
      "metadata": {
        "id": "uKaJB0bIYjj-"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the final score, you can modify the weight\n",
        "def final_score(score):\n",
        "  return 1.5*score[\"Language Clarity\"]+2*score[\"Content Accuracy\"]+score[\"Relevance to Role\"]+1.2*score[\"Emotional Intelligence\"]+1.0*score[\"Engagement Level\"] + 1.0*score[\"Personalization\"] + 1.5*score[\"Complexity Appropriateness\"] + 1.3*score[\"Ethical Sensitivity\"] + 0.8*score[\"Interactive Potential\"] + 1.0*score[\"Adaptability\"]"
      ],
      "metadata": {
        "id": "9u7PijBlZ2Vz"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# update all of record in json file\n",
        "def update_json_file(filename, new_data):\n",
        "    if os.path.exists(filename) and os.path.getsize(filename) > 0:\n",
        "        with open(filename, 'r') as file:\n",
        "            try:\n",
        "                data = json.load(file)\n",
        "            except json.JSONDecodeError:\n",
        "                data = []\n",
        "        data.append(new_data)\n",
        "        with open(filename, 'w') as file:\n",
        "          json.dump(data, file, indent=13)\n",
        "          # json.dump(new_data, file, indent=13)\n",
        "    else:\n",
        "        with open(filename, 'w') as file:\n",
        "          json.dump([new_data], file, indent=13)"
      ],
      "metadata": {
        "id": "nCvVtzMMc8Xd"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if final socre is less then therahold, it is unsatisfied. Else, satisfied\n",
        "therahold = 110\n",
        "random.shuffle(question_lst)\n",
        "i = 0\n",
        "question = question_lst[i]\n",
        "temp_q = None\n",
        "while i < len(question_lst):\n",
        "  student = openai.chat.completions.create(\n",
        "      temperature=0,\n",
        "      # limit the length of answer\n",
        "      max_tokens=100,\n",
        "      messages = [\n",
        "\n",
        "          {\"role\": \"user\", \"content\": question},\n",
        "      ],\n",
        "      model=\"gpt-4-0613\")\n",
        "\n",
        "  answer = student.choices[0].message.content\n",
        "  q_and_a = \"\"\"question is {question}\\n\n",
        "  Answer is {answer}\"\"\".format(question=question_lst[i],answer=answer)\n",
        "  grader = openai.chat.completions.create(\n",
        "      temperature=0,\n",
        "      messages = [\n",
        "\n",
        "          {\"role\": \"system\", \"content\": text_rule_grader},\n",
        "          {\"role\": \"user\",\"content\":q_and_a}\n",
        "      ],\n",
        "      model=\"gpt-4-0613\")\n",
        "  g = grader.choices[0].message.content\n",
        "  print(g)\n",
        "  q_a_dic = {\"Question\":question_lst[i], \"Answer\":answer}\n",
        "  print(q_a_dic)\n",
        "  the_dic = json.loads(g)\n",
        "  final_s = final_score(the_dic)\n",
        "  the_dic[\"final\"] = final_s\n",
        "  print(final_s)\n",
        "  q_a_dic.update(the_dic)\n",
        "  update_json_file(\"record.json\", q_a_dic)\n",
        "  if final_s < therahold:\n",
        "    question = generate_student_answer(question[i],answer,the_dic,therahold-final_s)\n",
        "  else:\n",
        "    i+=1\n",
        "    if i <= len(question_lst):\n",
        "      question = question_lst[i]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "LdK13GBSfnCC",
        "outputId": "33e8480e-0488-4fc6-9558-cd578e2b4c8f"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Water Conservation: Sustainable farming methods often use water more efficiently, which can help conserve this vital resource.\n",
            "\n",
            "4. Climate Change Mitigation: Sustainable farming can help mitigate climate change by sequestering carbon in the soil and reducing greenhouse gas emissions.\n",
            "\n",
            "5. Food Security: By improving soil health and biodiversity, sustainable farming can increase crop yields and resilience, helping to ensure a reliable food supply.\n",
            "\n",
            "{\"Language Clarity\": 9, \"Content Accuracy\": 10, \"Relevance to Role\": 10, \"Emotional Intelligence\": 8, \"Engagement Level\": 9, \"Personalization\": 7, \"Complexity Appropriateness\": 9,\"Ethical Sensitivity\": 10, \"Interactive Potential\": 8, \"Adaptability\": 9, \"final\": 92.6}\n",
            "{'Question': 'Advise a local farmer How can sustainable farming practices benefit our environment and food supply?', 'Answer': 'Sustainable farming practices can greatly benefit our environment and food supply in several ways:\\n\\n1. Soil Health: Sustainable farming practices such as crop rotation, cover cropping, and organic fertilizers can improve soil health by enhancing its fertility and preventing erosion. Healthy soil is crucial for growing nutritious and abundant crops.\\n\\n2. Biodiversity: Sustainable farming encourages biodiversity by providing habitats for a variety of plants and animals. This can help control pests and diseases naturally, reducing the need for harmful pesticides and chemicals.\\n\\n3.'}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "JSONDecodeError",
          "evalue": "Expecting value: line 1 column 1 (char 0)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-6c4a804657a1>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m   \u001b[0mq_a_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Question\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mquestion_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Answer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_a_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m   \u001b[0mthe_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m   \u001b[0mfinal_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mthe_dic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"final\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(int(the_dic[\"final\"]) < 104.55)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNdIuBaPftXm",
        "outputId": "0e52a190-4ae1-4e68-a52c-7988a5c0f068"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    }
  ]
}